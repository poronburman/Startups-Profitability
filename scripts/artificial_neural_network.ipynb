{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "773cd773c5cecd2f98b3b801285462107d16b9fec0e23178905f0577bd69827e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Startup Company's Profitability Prediction\n",
    "\n",
    "**Artificial Neural Network Regression**"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "> 1. The goal of this project is to train a model to predict the profit a startup company."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**1. Import libraries**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import tensorflow as tf "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 119,
   "outputs": []
  },
  {
   "source": [
    "**2. Import dataset**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*a) dataset*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0   165349.20       136897.80        471784.10    New York  192261.83\n",
       "1   162597.70       151377.59        443898.53  California  191792.06\n",
       "2   153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3   144372.41       118671.85        383199.62    New York  182901.99\n",
       "4   142107.34        91391.77        366168.42     Florida  166187.94\n",
       "5   131876.90        99814.71        362861.36    New York  156991.12\n",
       "6   134615.46       147198.87        127716.82  California  156122.51\n",
       "7   130298.13       145530.06        323876.68     Florida  155752.60\n",
       "8   120542.52       148718.95        311613.29    New York  152211.77\n",
       "9   123334.88       108679.17        304981.62  California  149759.96\n",
       "10  101913.08       110594.11        229160.95     Florida  146121.95\n",
       "11  100671.96        91790.61        249744.55  California  144259.40\n",
       "12   93863.75       127320.38        249839.44     Florida  141585.52\n",
       "13   91992.39       135495.07        252664.93  California  134307.35\n",
       "14  119943.24       156547.42        256512.92     Florida  132602.65\n",
       "15  114523.61       122616.84        261776.23    New York  129917.04\n",
       "16   78013.11       121597.55        264346.06  California  126992.93\n",
       "17   94657.16       145077.58        282574.31    New York  125370.37\n",
       "18   91749.16       114175.79        294919.57     Florida  124266.90\n",
       "19   86419.70       153514.11             0.00    New York  122776.86\n",
       "20   76253.86       113867.30        298664.47  California  118474.03\n",
       "21   78389.47       153773.43        299737.29    New York  111313.02\n",
       "22   73994.56       122782.75        303319.26     Florida  110352.25\n",
       "23   67532.53       105751.03        304768.73     Florida  108733.99\n",
       "24   77044.01        99281.34        140574.81    New York  108552.04\n",
       "25   64664.71       139553.16        137962.62  California  107404.34\n",
       "26   75328.87       144135.98        134050.07     Florida  105733.54\n",
       "27   72107.60       127864.55        353183.81    New York  105008.31\n",
       "28   66051.52       182645.56        118148.20     Florida  103282.38\n",
       "29   65605.48       153032.06        107138.38    New York  101004.64\n",
       "30   61994.48       115641.28         91131.24     Florida   99937.59\n",
       "31   61136.38       152701.92         88218.23    New York   97483.56\n",
       "32   63408.86       129219.61         46085.25  California   97427.84\n",
       "33   55493.95       103057.49        214634.81     Florida   96778.92\n",
       "34   46426.07       157693.92        210797.67  California   96712.80\n",
       "35   46014.02        85047.44        205517.64    New York   96479.51\n",
       "36   28663.76       127056.21        201126.82     Florida   90708.19\n",
       "37   44069.95        51283.14        197029.42  California   89949.14\n",
       "38   20229.59        65947.93        185265.10    New York   81229.06\n",
       "39   38558.51        82982.09        174999.30  California   81005.76\n",
       "40   28754.33       118546.05        172795.67  California   78239.91\n",
       "41   27892.92        84710.77        164470.71     Florida   77798.83\n",
       "42   23640.93        96189.63        148001.11  California   71498.49\n",
       "43   15505.73       127382.30         35534.17    New York   69758.98\n",
       "44   22177.74       154806.14         28334.72  California   65200.33\n",
       "45    1000.23       124153.04          1903.93    New York   64926.08\n",
       "46    1315.46       115816.21        297114.46     Florida   49490.75\n",
       "47       0.00       135426.92             0.00  California   42559.73\n",
       "48     542.05        51743.15             0.00    New York   35673.41\n",
       "49       0.00       116983.80         45173.06  California   14681.40"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R&amp;D Spend</th>\n      <th>Administration</th>\n      <th>Marketing Spend</th>\n      <th>State</th>\n      <th>Profit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165349.20</td>\n      <td>136897.80</td>\n      <td>471784.10</td>\n      <td>New York</td>\n      <td>192261.83</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>162597.70</td>\n      <td>151377.59</td>\n      <td>443898.53</td>\n      <td>California</td>\n      <td>191792.06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>153441.51</td>\n      <td>101145.55</td>\n      <td>407934.54</td>\n      <td>Florida</td>\n      <td>191050.39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>144372.41</td>\n      <td>118671.85</td>\n      <td>383199.62</td>\n      <td>New York</td>\n      <td>182901.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>142107.34</td>\n      <td>91391.77</td>\n      <td>366168.42</td>\n      <td>Florida</td>\n      <td>166187.94</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>131876.90</td>\n      <td>99814.71</td>\n      <td>362861.36</td>\n      <td>New York</td>\n      <td>156991.12</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>134615.46</td>\n      <td>147198.87</td>\n      <td>127716.82</td>\n      <td>California</td>\n      <td>156122.51</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>130298.13</td>\n      <td>145530.06</td>\n      <td>323876.68</td>\n      <td>Florida</td>\n      <td>155752.60</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>120542.52</td>\n      <td>148718.95</td>\n      <td>311613.29</td>\n      <td>New York</td>\n      <td>152211.77</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>123334.88</td>\n      <td>108679.17</td>\n      <td>304981.62</td>\n      <td>California</td>\n      <td>149759.96</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>101913.08</td>\n      <td>110594.11</td>\n      <td>229160.95</td>\n      <td>Florida</td>\n      <td>146121.95</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>100671.96</td>\n      <td>91790.61</td>\n      <td>249744.55</td>\n      <td>California</td>\n      <td>144259.40</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>93863.75</td>\n      <td>127320.38</td>\n      <td>249839.44</td>\n      <td>Florida</td>\n      <td>141585.52</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>91992.39</td>\n      <td>135495.07</td>\n      <td>252664.93</td>\n      <td>California</td>\n      <td>134307.35</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>119943.24</td>\n      <td>156547.42</td>\n      <td>256512.92</td>\n      <td>Florida</td>\n      <td>132602.65</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>114523.61</td>\n      <td>122616.84</td>\n      <td>261776.23</td>\n      <td>New York</td>\n      <td>129917.04</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>78013.11</td>\n      <td>121597.55</td>\n      <td>264346.06</td>\n      <td>California</td>\n      <td>126992.93</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>94657.16</td>\n      <td>145077.58</td>\n      <td>282574.31</td>\n      <td>New York</td>\n      <td>125370.37</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>91749.16</td>\n      <td>114175.79</td>\n      <td>294919.57</td>\n      <td>Florida</td>\n      <td>124266.90</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>86419.70</td>\n      <td>153514.11</td>\n      <td>0.00</td>\n      <td>New York</td>\n      <td>122776.86</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>76253.86</td>\n      <td>113867.30</td>\n      <td>298664.47</td>\n      <td>California</td>\n      <td>118474.03</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>78389.47</td>\n      <td>153773.43</td>\n      <td>299737.29</td>\n      <td>New York</td>\n      <td>111313.02</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>73994.56</td>\n      <td>122782.75</td>\n      <td>303319.26</td>\n      <td>Florida</td>\n      <td>110352.25</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>67532.53</td>\n      <td>105751.03</td>\n      <td>304768.73</td>\n      <td>Florida</td>\n      <td>108733.99</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>77044.01</td>\n      <td>99281.34</td>\n      <td>140574.81</td>\n      <td>New York</td>\n      <td>108552.04</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>64664.71</td>\n      <td>139553.16</td>\n      <td>137962.62</td>\n      <td>California</td>\n      <td>107404.34</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>75328.87</td>\n      <td>144135.98</td>\n      <td>134050.07</td>\n      <td>Florida</td>\n      <td>105733.54</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>72107.60</td>\n      <td>127864.55</td>\n      <td>353183.81</td>\n      <td>New York</td>\n      <td>105008.31</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>66051.52</td>\n      <td>182645.56</td>\n      <td>118148.20</td>\n      <td>Florida</td>\n      <td>103282.38</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>65605.48</td>\n      <td>153032.06</td>\n      <td>107138.38</td>\n      <td>New York</td>\n      <td>101004.64</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>61994.48</td>\n      <td>115641.28</td>\n      <td>91131.24</td>\n      <td>Florida</td>\n      <td>99937.59</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>61136.38</td>\n      <td>152701.92</td>\n      <td>88218.23</td>\n      <td>New York</td>\n      <td>97483.56</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>63408.86</td>\n      <td>129219.61</td>\n      <td>46085.25</td>\n      <td>California</td>\n      <td>97427.84</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>55493.95</td>\n      <td>103057.49</td>\n      <td>214634.81</td>\n      <td>Florida</td>\n      <td>96778.92</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>46426.07</td>\n      <td>157693.92</td>\n      <td>210797.67</td>\n      <td>California</td>\n      <td>96712.80</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>46014.02</td>\n      <td>85047.44</td>\n      <td>205517.64</td>\n      <td>New York</td>\n      <td>96479.51</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>28663.76</td>\n      <td>127056.21</td>\n      <td>201126.82</td>\n      <td>Florida</td>\n      <td>90708.19</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>44069.95</td>\n      <td>51283.14</td>\n      <td>197029.42</td>\n      <td>California</td>\n      <td>89949.14</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>20229.59</td>\n      <td>65947.93</td>\n      <td>185265.10</td>\n      <td>New York</td>\n      <td>81229.06</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>38558.51</td>\n      <td>82982.09</td>\n      <td>174999.30</td>\n      <td>California</td>\n      <td>81005.76</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>28754.33</td>\n      <td>118546.05</td>\n      <td>172795.67</td>\n      <td>California</td>\n      <td>78239.91</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>27892.92</td>\n      <td>84710.77</td>\n      <td>164470.71</td>\n      <td>Florida</td>\n      <td>77798.83</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>23640.93</td>\n      <td>96189.63</td>\n      <td>148001.11</td>\n      <td>California</td>\n      <td>71498.49</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>15505.73</td>\n      <td>127382.30</td>\n      <td>35534.17</td>\n      <td>New York</td>\n      <td>69758.98</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>22177.74</td>\n      <td>154806.14</td>\n      <td>28334.72</td>\n      <td>California</td>\n      <td>65200.33</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>1000.23</td>\n      <td>124153.04</td>\n      <td>1903.93</td>\n      <td>New York</td>\n      <td>64926.08</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>1315.46</td>\n      <td>115816.21</td>\n      <td>297114.46</td>\n      <td>Florida</td>\n      <td>49490.75</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.00</td>\n      <td>135426.92</td>\n      <td>0.00</td>\n      <td>California</td>\n      <td>42559.73</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>542.05</td>\n      <td>51743.15</td>\n      <td>0.00</td>\n      <td>New York</td>\n      <td>35673.41</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.00</td>\n      <td>116983.80</td>\n      <td>45173.06</td>\n      <td>California</td>\n      <td>14681.40</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "source": [
    "*b) check for any null values*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "dataset.isnull().values.any()"
   ]
  },
  {
   "source": [
    "**3. Data preprocessing**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*a) select dependent and independent variables*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "source": [
    "*b) encode categorical data*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00e+00, 0.00e+00, 1.00e+00, 1.65e+05, 1.37e+05, 4.72e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.63e+05, 1.51e+05, 4.44e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.53e+05, 1.01e+05, 4.08e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.44e+05, 1.19e+05, 3.83e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.42e+05, 9.14e+04, 3.66e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.32e+05, 9.98e+04, 3.63e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.35e+05, 1.47e+05, 1.28e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.30e+05, 1.46e+05, 3.24e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.21e+05, 1.49e+05, 3.12e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.23e+05, 1.09e+05, 3.05e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.02e+05, 1.11e+05, 2.29e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.01e+05, 9.18e+04, 2.50e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 9.39e+04, 1.27e+05, 2.50e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 9.20e+04, 1.35e+05, 2.53e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.20e+05, 1.57e+05, 2.57e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.15e+05, 1.23e+05, 2.62e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 7.80e+04, 1.22e+05, 2.64e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 9.47e+04, 1.45e+05, 2.83e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 9.17e+04, 1.14e+05, 2.95e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 8.64e+04, 1.54e+05, 0.00e+00],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 7.63e+04, 1.14e+05, 2.99e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 7.84e+04, 1.54e+05, 3.00e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 7.40e+04, 1.23e+05, 3.03e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 6.75e+04, 1.06e+05, 3.05e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 7.70e+04, 9.93e+04, 1.41e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 6.47e+04, 1.40e+05, 1.38e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 7.53e+04, 1.44e+05, 1.34e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 7.21e+04, 1.28e+05, 3.53e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 6.61e+04, 1.83e+05, 1.18e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 6.56e+04, 1.53e+05, 1.07e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 6.20e+04, 1.16e+05, 9.11e+04],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 6.11e+04, 1.53e+05, 8.82e+04],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 6.34e+04, 1.29e+05, 4.61e+04],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 5.55e+04, 1.03e+05, 2.15e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 4.64e+04, 1.58e+05, 2.11e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 4.60e+04, 8.50e+04, 2.06e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 2.87e+04, 1.27e+05, 2.01e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 4.41e+04, 5.13e+04, 1.97e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 2.02e+04, 6.59e+04, 1.85e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 3.86e+04, 8.30e+04, 1.75e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 2.88e+04, 1.19e+05, 1.73e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 2.79e+04, 8.47e+04, 1.64e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 2.36e+04, 9.62e+04, 1.48e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.55e+04, 1.27e+05, 3.55e+04],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 2.22e+04, 1.55e+05, 2.83e+04],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.00e+03, 1.24e+05, 1.90e+03],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.32e+03, 1.16e+05, 2.97e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 1.35e+05, 0.00e+00],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 5.42e+02, 5.17e+04, 0.00e+00],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 1.17e+05, 4.52e+04]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
    "\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "X = np.asarray(X).astype('float32')\n",
    "\n",
    "X"
   ]
  },
  {
   "source": [
    "*c) split dataset into training and testing*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "source": [
    "*d) view X_train*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00e+00, 1.00e+00, 0.00e+00, 5.55e+04, 1.03e+05, 2.15e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 4.60e+04, 8.50e+04, 2.06e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 7.53e+04, 1.44e+05, 1.34e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 4.64e+04, 1.58e+05, 2.11e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 9.17e+04, 1.14e+05, 2.95e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.30e+05, 1.46e+05, 3.24e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.20e+05, 1.57e+05, 2.57e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.00e+03, 1.24e+05, 1.90e+03],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 5.42e+02, 5.17e+04, 0.00e+00],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 6.56e+04, 1.53e+05, 1.07e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.15e+05, 1.23e+05, 2.62e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 6.20e+04, 1.16e+05, 9.11e+04],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 6.34e+04, 1.29e+05, 4.61e+04],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 7.80e+04, 1.22e+05, 2.64e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 2.36e+04, 9.62e+04, 1.48e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 7.63e+04, 1.14e+05, 2.99e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.55e+04, 1.27e+05, 3.55e+04],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.21e+05, 1.49e+05, 3.12e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 9.20e+04, 1.35e+05, 2.53e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 6.47e+04, 1.40e+05, 1.38e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.32e+05, 9.98e+04, 3.63e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 9.47e+04, 1.45e+05, 2.83e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 2.88e+04, 1.19e+05, 1.73e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 1.17e+05, 4.52e+04],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.63e+05, 1.51e+05, 4.44e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 9.39e+04, 1.27e+05, 2.50e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 4.41e+04, 5.13e+04, 1.97e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 7.70e+04, 9.93e+04, 1.41e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.35e+05, 1.47e+05, 1.28e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 6.75e+04, 1.06e+05, 3.05e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 2.87e+04, 1.27e+05, 2.01e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 7.84e+04, 1.54e+05, 3.00e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 8.64e+04, 1.54e+05, 0.00e+00],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.23e+05, 1.09e+05, 3.05e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 3.86e+04, 8.30e+04, 1.75e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.32e+03, 1.16e+05, 2.97e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.44e+05, 1.19e+05, 3.83e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 1.65e+05, 1.37e+05, 4.72e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 1.35e+05, 0.00e+00],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 2.22e+04, 1.55e+05, 2.83e+04]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "source": [
    "*e) view X_test*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00e+00, 1.00e+00, 0.00e+00, 6.61e+04, 1.83e+05, 1.18e+05],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, 1.01e+05, 9.18e+04, 2.50e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.02e+05, 1.11e+05, 2.29e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 2.79e+04, 8.47e+04, 1.64e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.53e+05, 1.01e+05, 4.08e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 7.21e+04, 1.28e+05, 3.53e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 2.02e+04, 6.59e+04, 1.85e+05],\n",
       "       [0.00e+00, 0.00e+00, 1.00e+00, 6.11e+04, 1.53e+05, 8.82e+04],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 7.40e+04, 1.23e+05, 3.03e+05],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, 1.42e+05, 9.14e+04, 3.66e+05]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "source": [
    "*f) view y_train*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 96778.92,  96479.51, 105733.54,  96712.8 , 124266.9 , 155752.6 ,\n",
       "       132602.65,  64926.08,  35673.41, 101004.64, 129917.04,  99937.59,\n",
       "        97427.84, 126992.93,  71498.49, 118474.03,  69758.98, 152211.77,\n",
       "       134307.35, 107404.34, 156991.12, 125370.37,  78239.91,  14681.4 ,\n",
       "       191792.06, 141585.52,  89949.14, 108552.04, 156122.51, 108733.99,\n",
       "        90708.19, 111313.02, 122776.86, 149759.96,  81005.76,  49490.75,\n",
       "       182901.99, 192261.83,  42559.73,  65200.33])"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "source": [
    "*g) view y_test*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([103282.38, 144259.4 , 146121.95,  77798.83, 191050.39, 105008.31,\n",
       "        81229.06,  97483.56, 110352.25, 166187.94])"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "source": [
    "**4. Artificial Neural Network**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*a) model*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "source": [
    "*b) add first hidden layer*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=10, activation='relu'))"
   ]
  },
  {
   "source": [
    "*c) add second hidden layer*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=10, activation='relu'))"
   ]
  },
  {
   "source": [
    "*d) add output layer*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1))"
   ]
  },
  {
   "source": [
    "*e) compile the ann*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "source": [
    "*f) train the model on the training set*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================] - 0s 1ms/step - loss: 153394208.0000\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 153208272.0000\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 152980816.0000\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 961us/step - loss: 152865312.0000\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 152500624.0000\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 152440944.0000\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 989us/step - loss: 152574848.0000\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 152082496.0000\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 151830176.0000\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 723us/step - loss: 151570640.0000\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 964us/step - loss: 151503056.0000\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 849us/step - loss: 151212448.0000\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 841us/step - loss: 151042304.0000\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 706us/step - loss: 151022592.0000\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 817us/step - loss: 150802208.0000\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 150639312.0000\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 910us/step - loss: 150545120.0000\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 766us/step - loss: 150322256.0000\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 941us/step - loss: 150171168.0000\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 846us/step - loss: 149972944.0000\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 839us/step - loss: 149879168.0000\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 899us/step - loss: 150038592.0000\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 881us/step - loss: 149752784.0000\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 908us/step - loss: 149578528.0000\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 149457616.0000\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 149404192.0000\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 149222800.0000\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 819us/step - loss: 149070112.0000\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 752us/step - loss: 149402528.0000\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 741us/step - loss: 149195904.0000\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 148932624.0000\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 947us/step - loss: 148735664.0000\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 705us/step - loss: 148572544.0000\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 796us/step - loss: 148293920.0000\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 148077536.0000\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 147916336.0000\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 147726768.0000\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 147685424.0000\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 147556832.0000\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 147555184.0000\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 147455584.0000\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 147302016.0000\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 147081184.0000\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 146881552.0000\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 147164336.0000\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 147234176.0000\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 968us/step - loss: 147195920.0000\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 147108592.0000\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 146947280.0000\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 936us/step - loss: 146779504.0000\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 851us/step - loss: 146679936.0000\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 146484640.0000\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 146232160.0000\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 921us/step - loss: 145754320.0000\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 919us/step - loss: 145774832.0000\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 939us/step - loss: 145185376.0000\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 144841568.0000\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 144952320.0000\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 876us/step - loss: 144602976.0000\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 144534848.0000\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 144403728.0000\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 144312912.0000\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 144195584.0000\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 143909008.0000\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 143649952.0000\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 739us/step - loss: 143608432.0000\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 878us/step - loss: 144342976.0000\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 679us/step - loss: 144268320.0000\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 145060560.0000\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 144864080.0000\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 144480768.0000\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 828us/step - loss: 144125728.0000\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 888us/step - loss: 143784608.0000\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 764us/step - loss: 143202368.0000\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 142965280.0000\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 142633824.0000\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 142490432.0000\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 142376320.0000\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 142241104.0000\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 142223504.0000\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 142232608.0000\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 142325104.0000\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 142577104.0000\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 802us/step - loss: 142599008.0000\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 142313520.0000\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 885us/step - loss: 141722528.0000\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 141597232.0000\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 141309808.0000\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 141404112.0000\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 141458800.0000\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 141485408.0000\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 987us/step - loss: 141446320.0000\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 141472096.0000\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 141299664.0000\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 141088992.0000\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 895us/step - loss: 140924896.0000\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 141047840.0000\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 140793184.0000\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 140881200.0000\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 140940256.0000\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 141044112.0000\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 792us/step - loss: 141416320.0000\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 141719392.0000\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 141615200.0000\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 141478496.0000\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 141275696.0000\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 140650688.0000\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 652us/step - loss: 140489568.0000\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 779us/step - loss: 140032640.0000\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 139864784.0000\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 139654688.0000\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 139490992.0000\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 139282848.0000\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 995us/step - loss: 139187584.0000\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 139127904.0000\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 139196944.0000\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 138832720.0000\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 805us/step - loss: 138889696.0000\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 138893152.0000\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138658032.0000\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 138544432.0000\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138406704.0000\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138342240.0000\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138195152.0000\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138141104.0000\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 138095696.0000\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138260656.0000\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138285504.0000\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 138430528.0000\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 719us/step - loss: 138491872.0000\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 138281792.0000\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 837us/step - loss: 138084816.0000\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 137650736.0000\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 137304368.0000\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 136629984.0000\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 136920688.0000\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 137106144.0000\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 137666912.0000\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 137564608.0000\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 949us/step - loss: 137176288.0000\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 136880176.0000\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 136354704.0000\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 136167872.0000\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 136352176.0000\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 136366848.0000\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 137010912.0000\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 137828384.0000\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 138583264.0000\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 138698096.0000\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 138063568.0000\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 137017424.0000\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 136978720.0000\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 135865456.0000\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 135880704.0000\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 135714512.0000\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 135627312.0000\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 135584384.0000\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 135354064.0000\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 745us/step - loss: 134996688.0000\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 134818768.0000\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 686us/step - loss: 134690000.0000\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 672us/step - loss: 134730784.0000\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 134831904.0000\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 135051936.0000\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 894us/step - loss: 135206976.0000\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 135362400.0000\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 135321952.0000\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 135144544.0000\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 134972880.0000\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 134971552.0000\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 134763008.0000\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 134496704.0000\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 134032816.0000\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 133761640.0000\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 133963632.0000\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 133921880.0000\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 133849408.0000\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 133895256.0000\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 134106832.0000\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 134814880.0000\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 135595296.0000\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 135585024.0000\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 135261024.0000\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 134975456.0000\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 134703648.0000\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 768us/step - loss: 134381200.0000\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 133934352.0000\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 133580632.0000\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 760us/step - loss: 132995608.0000\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 722us/step - loss: 132589040.0000\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 132813392.0000\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 132395712.0000\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 132257384.0000\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 132084888.0000\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 698us/step - loss: 132302768.0000\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 777us/step - loss: 132497200.0000\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 132572784.0000\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 133363392.0000\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 133214120.0000\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 132852504.0000\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 132499712.0000\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 131975424.0000\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 131737240.0000\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 131560040.0000\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 842us/step - loss: 131574720.0000\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 131479400.0000\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 131659760.0000\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 131507792.0000\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 747us/step - loss: 131666448.0000\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 131384728.0000\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 131665872.0000\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 935us/step - loss: 131490712.0000\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 131474192.0000\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 131284672.0000\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 759us/step - loss: 131320640.0000\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 757us/step - loss: 131230824.0000\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 721us/step - loss: 131152144.0000\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 864us/step - loss: 130991792.0000\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 662us/step - loss: 130966552.0000\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 759us/step - loss: 130872360.0000\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 640us/step - loss: 131107472.0000\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 728us/step - loss: 131300656.0000\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 131102952.0000\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 857us/step - loss: 130905616.0000\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 845us/step - loss: 130705576.0000\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 130689112.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcbe4e4f2e0>"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size=32, epochs=500)"
   ]
  },
  {
   "source": [
    "*c) predict the test set*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:7 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcbe4e5d160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[112998.75],\n",
       "       [119878.73],\n",
       "       [122735.11],\n",
       "       [ 63101.04],\n",
       "       [176063.42],\n",
       "       [125607.37],\n",
       "       [ 53937.87],\n",
       "       [ 99830.48],\n",
       "       [120054.12],\n",
       "       [160987.17]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "*d) compare the predicted results with the original y_test values*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[112998.75 103282.38]\n [119878.73 144259.4 ]\n [122735.11 146121.95]\n [ 63101.04  77798.83]\n [176063.42 191050.39]\n [125607.37 105008.31]\n [ 53937.87  81229.06]\n [ 99830.48  97483.56]\n [120054.12 110352.25]\n [160987.17 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "source": [
    "*e) evaluate the model's performance*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7675943798978109"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}